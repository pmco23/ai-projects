{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oqQVuPlgiuHT",
        "xuP1hxQhxVXT"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsBl5YiPpYeO5EP9Tdvq20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmco23/ai-projects/blob/main/ai_projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Training"
      ],
      "metadata": {
        "id": "GjXIDFlQtwV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to ai pipelines"
      ],
      "metadata": {
        "id": "oqQVuPlgiuHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All the available pipelines\n",
        "\n",
        "Here are all the pipelines available from Transformers and Diffusers.\n",
        "\n",
        "There's a list pipelines under the Tasks on this page (you have to scroll down a bit, then expand the parameters to see the Tasks):\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "There's also this list of Tasks for Diffusion models instead of Transformers, following the image generation example where I use DiffusionPipeline above.\n",
        "\n",
        "https://huggingface.co/docs/diffusers/en/api/pipelines/overview\n"
      ],
      "metadata": {
        "id": "Ta6MtKA9ned0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Examples"
      ],
      "metadata": {
        "id": "4ITFBgk4oOfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if this gives an \"ERROR\" about pip dependency conflicts, ignore it! It doesn't affect anything.\n",
        "!pip install -q -U transformers datasets diffusers"
      ],
      "metadata": {
        "id": "BwM5OqUJi9m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "from transformers.pipelines.token_classification import AggregationStrategy\n",
        "\n",
        "hf_token = userdata.get('HF_API_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "nDhVCiGKjKQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"I'm super excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "zJjYH3O_jmtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Name Entity Recognition\n",
        "ner = pipeline(\"ner\", aggregation_strategy=AggregationStrategy.SIMPLE, device=\"cuda\")\n",
        "result = ner(\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "m684zmqilRDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question Answering with Context\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Nz7LPZYelaYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Summarization\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "OS79BkzDmAML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ],
      "metadata": {
        "id": "vWc1yMxYncH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "yYvEqniFoYLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "dnIaBIvCuuOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Generation\n",
        "\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"A bunch of Devops Engineers trying to solve a problem. The image should be in japonese manga style\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "4RGFd1qpsJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Generation\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ],
      "metadata": {
        "id": "J0ChH17gtSze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizers\n"
      ],
      "metadata": {
        "id": "xuP1hxQhxVXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "pgIfU4njxfuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "xvQ0VDmWxgd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_API_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "67KndfGxxpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try llama3.1 model\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "XZmhN-6vxuxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "NCyk2ZS4x3_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "4nuxsumLybgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "iUXt8HNPyd2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "sehdq6ZlyhiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab\n",
        "#tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "OJEkkotXyka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruct model have been trained to expect prompts with a particular format that includes system, user and assistant prompts.\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "mv3787Cj2ac-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try other models\n",
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "VHYO47vP1khA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "#print(tokenizer.encode(text))\n",
        "#print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print()\n",
        "print(phi3_tokenizer.batch_decode(tokens))\n",
        "\n"
      ],
      "metadata": {
        "id": "JQoQSJlQ1sxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "#print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "#print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "fyk4aipo2-1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "#print(tokenizer.encode(text))\n",
        "#print()\n",
        "#print(phi3_tokenizer.encode(text))\n",
        "#print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "wi6gn6RI3PzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "# print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "# print()\n",
        "# print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "# print()\n",
        "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "FoLbIM5_3jiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = starcoder2_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "MHzQxqya332a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n"
      ],
      "metadata": {
        "id": "GukdGkiUhYJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0"
      ],
      "metadata": {
        "id": "g2Q6ZnUUhkqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "G9DXR2REh8Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_API_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "D5lHxQk4iAJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instruct models\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\"\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
      ],
      "metadata": {
        "id": "DRad1XVkiMie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "MVXrnlzkiRIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "BuXjkpug22-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "K15gum1-2Aau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "metadata": {
        "id": "gCWlk9_94JXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ],
      "metadata": {
        "id": "WviSTOnvAf5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking under the hood at the Transformer model\n",
        "\n",
        "The next cell prints the HuggingFace `model` object for Llama.\n",
        "\n",
        "This model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n",
        "\n",
        "[Introduction to neural networks](https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs)\n",
        "\n",
        "Now take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n",
        "\n",
        "- It consists of layers\n",
        "- There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors.\n",
        "- There are then 32 sets of groups of layers called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n",
        "- There is an LM Head layer at the end; this produces the output\n",
        "\n",
        "Notice the mention that the model has been quantized to 4 bits.\n",
        "\n"
      ],
      "metadata": {
        "id": "P-QJZDfEAsua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b\n",
        "# Execute this cell and look at what gets printed; investigate the layers\n",
        "model"
      ],
      "metadata": {
        "id": "r1Rmw5_VAhee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OK, with that, now let's run the model!\n",
        "\n",
        "outputs = model.generate(inputs, max_new_tokens=80)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "bg9VkiEHBjYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up memory\n",
        "del model, inputs, tokenizer, outputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3dQV3hAzB8YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=1000, streamer=streamer)\n",
        "  del model, inputs, tokenizer, outputs, streamer\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FIeWz7YHCDrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Create a little introduction to pipelines,models,and tokenizers and how to use it in AI\"}\n",
        "  ]\n",
        "generate(LLAMA, messages)"
      ],
      "metadata": {
        "id": "eL8FGGFBCLDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation audio to text minutes"
      ],
      "metadata": {
        "id": "48NBGUuKSRgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
      ],
      "metadata": {
        "id": "qFi6vU3YSZ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig,AutoModelForSpeechSeq2Seq,AutoProcessor,pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "oYs2wrLeSctv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "AUDIO_MODEL = \"whisper-1\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "X57BdiPYSofJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New capability - connect this Colab to my Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "audio_filename = '/content/drive/MyDrive/Colab Notebooks/denver_extract.mp3'"
      ],
      "metadata": {
        "id": "2OHURTYOS01x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "hf_token = userdata.get('HF_API_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "Cv3tnCUXTwIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MODEL = \"openai/whisper-medium\"\n",
        "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n",
        "speech_model.to('cuda')\n",
        "processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=speech_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch.float16,\n",
        "    device='cuda',\n",
        "    return_timestamps=True,\n",
        "    chunk_length_s=30,\n",
        ")"
      ],
      "metadata": {
        "id": "E8SrhFggT1Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Whisper OpenAI model to convert the Audio to Text\n",
        "result = pipe(audio_filename)"
      ],
      "metadata": {
        "id": "_H9TLBp8UeVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = result[\"text\"]\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "qLNpxxeHUfFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "lWBIQT6xWl5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "Q-JOp-lZWnYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_minute(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "  return tokenizer.decode(outputs[0])\n",
        "  del model, inputs, tokenizer, outputs, streamer\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "T3N_vmbKWq6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_minute(LLAMA, messages)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "id": "3MubjXmLXEIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate datasets"
      ],
      "metadata": {
        "id": "0fuvzHdGaeoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
      ],
      "metadata": {
        "id": "fCyHMXv4aeL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "hf_token = userdata.get('HF_API_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "QfkD5lzkauV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  device_map=\"auto\",\n",
        "  quantization_config=quant_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "gYwEnF9Wa3qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3):\n",
        "    # Convert user inputs into multi-shot examples\n",
        "    multi_shot_examples = [\n",
        "        {\"instruction\": inst1, \"response\": resp1},\n",
        "        {\"instruction\": inst2, \"response\": resp2},\n",
        "        {\"instruction\": inst3, \"response\": resp3}\n",
        "    ]\n",
        "\n",
        "    # System prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a helpful assistant whose main purpose is to generate datasets.\n",
        "    Topic: {topic}\n",
        "    Return the dataset in JSON format. Use examples with simple, fun, and easy-to-understand instructions for kids.\n",
        "    Include the following examples: {multi_shot_examples}\n",
        "    Return {number_of_data} examples each time.\n",
        "    Do not repeat the provided examples.\n",
        "    \"\"\"\n",
        "\n",
        "    # Example Messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Please generate my dataset for {topic}\"}\n",
        "    ]\n",
        "\n",
        "    # Tokenize Input\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Generate Output\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "\n",
        "    # Decode and Return\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def gradio_interface(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3):\n",
        "    return generate_dataset(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3)"
      ],
      "metadata": {
        "id": "JgdsUZLua-lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_topic = \"Talking to a (5-8) years old and teaching them manners.\"\n",
        "default_number_of_data = 10\n",
        "default_multi_shot_examples = [\n",
        "    {\n",
        "        \"instruction\": \"Why do I have to say please when I want something?\",\n",
        "        \"response\": \"Because it’s like magic! It shows you’re nice, and people want to help you more.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What should I say if someone gives me a toy?\",\n",
        "        \"response\": \"You say, 'Thank you!' because it makes them happy you liked it.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"why should I listen to my parents?\",\n",
        "        \"response\": \"Because parents want the best for you and they love you the most.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "acTRE7oVbNXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Topic\", value=default_topic, lines=2),\n",
        "        gr.Number(label=\"Number of Examples\", value=default_number_of_data, precision=0),\n",
        "        gr.Textbox(label=\"Instruction 1\", value=default_multi_shot_examples[0][\"instruction\"]),\n",
        "        gr.Textbox(label=\"Response 1\", value=default_multi_shot_examples[0][\"response\"]),\n",
        "        gr.Textbox(label=\"Instruction 2\", value=default_multi_shot_examples[1][\"instruction\"]),\n",
        "        gr.Textbox(label=\"Response 2\", value=default_multi_shot_examples[1][\"response\"]),\n",
        "        gr.Textbox(label=\"Instruction 3\", value=default_multi_shot_examples[2][\"instruction\"]),\n",
        "        gr.Textbox(label=\"Response 3\", value=default_multi_shot_examples[2][\"response\"]),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Dataset\")\n",
        ")"
      ],
      "metadata": {
        "id": "vKR_z_4ebOU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_interface.launch()"
      ],
      "metadata": {
        "id": "W2FC7icQbTrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}